{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "random_seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_mode, except_num, radius=2,\n",
    "                  center=(0, 0), sigma=0.1, num_data_per_class=100000):\n",
    "    total_data = {}\n",
    "    \n",
    "    t = np.linspace(0, 2*np.pi, 13)\n",
    "    x = np.cos(t)*radius + center[0]\n",
    "    y = np.sin(t)*radius + center[1]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x,y)\n",
    "\n",
    "    modes = np.vstack([x, y]).T\n",
    "\n",
    "    for idx, mode in enumerate(modes[except_num:]):\n",
    "        x = np.random.normal(mode[0], sigma, num_data_per_class)\n",
    "        y = np.random.normal(mode[1], sigma, num_data_per_class)\n",
    "        total_data[idx] = np.vstack([x, y]).T\n",
    "        \n",
    "        plt.plot(x, y)\n",
    "\n",
    "    all_points = np.vstack([values for values in total_data.values()])\n",
    "    data_x, data_y = all_points[:,0], all_points[:,1]\n",
    "    \n",
    "    return total_data, all_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_data_with_class, A_data = generate_data(13, 3, radius=2, center=(-2, -2))\n",
    "B_data_with_class, B_data = generate_data(13, 3, radius=2, center=(2, 2))\n",
    "\n",
    "A_train_np, A_test_np = train_test_split(A_data, test_size=0.33, random_state=random_seed)\n",
    "B_train_np, B_test_np = train_test_split(B_data, test_size=0.33, random_state=random_seed)\n",
    "\n",
    "def plot(data, color):\n",
    "    plt.plot(data[:,0], data[:,1], color)\n",
    "\n",
    "plt.figure()\n",
    "#sns.kdeplot(data_x, data_y)\n",
    "plot(A_data, 'r.')\n",
    "plot(B_data, 'b.')\n",
    "\n",
    "def plot_with_class(data_with_class):\n",
    "    for key, value in data_with_class.items():\n",
    "        plot(value, '.')\n",
    "\n",
    "plt.figure()\n",
    "plot_with_class(A_data_with_class)\n",
    "plot_with_class(B_data_with_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "class ListModule(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(ListModule, self).__init__()\n",
    "        idx = 0\n",
    "        for module in args:\n",
    "            self.add_module(str(idx), module)\n",
    "            idx += 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= len(self._modules):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        it = iter(self._modules.values())\n",
    "        for i in range(idx):\n",
    "            next(it)\n",
    "        return next(it)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader\n",
    "batch_size = 200\n",
    "shuffle = False\n",
    "\n",
    "A_train, A_test = torch.from_numpy(A_train_np).float(), torch.from_numpy(A_test_np).float()\n",
    "B_train, B_test = torch.from_numpy(B_train_np).float(), torch.from_numpy(B_test_np).float()\n",
    "\n",
    "A_train_loader = DataLoader(TensorDataset(A_train, A_train), batch_size=batch_size, shuffle=shuffle)\n",
    "A_test_loader = DataLoader(TensorDataset(A_test, A_test), batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "B_train_loader = DataLoader(TensorDataset(B_train, B_train), batch_size=batch_size, shuffle=shuffle)\n",
    "B_test_loader = DataLoader(TensorDataset(B_test, B_test), batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dims):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layers = []\n",
    "        \n",
    "        prev_dim = input_size\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            self.layers.append(nn.ReLU(True))\n",
    "            prev_dim = hidden_dim\n",
    "        self.layers.append(nn.Linear(prev_dim, output_size))\n",
    "        \n",
    "        self.layer_module = ListModule(*self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dims):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layers = []\n",
    "        \n",
    "        prev_dim = input_size\n",
    "        for idx, hidden_dim in enumerate(hidden_dims):\n",
    "            self.layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            self.layers.append(nn.ReLU(True))\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        self.layers.append(nn.Linear(prev_dim, output_size))\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.layer_module = ListModule(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out.view(-1, 1)\n",
    "\n",
    "# network\n",
    "hidden_dim = 128\n",
    "g_num_layer = 3\n",
    "d_num_layer = 5\n",
    "\n",
    "G_AB = Generator(2, 2, [hidden_dim] * g_num_layer)\n",
    "G_BA = Generator(2, 2, [hidden_dim] * g_num_layer)\n",
    "\n",
    "D_A = Discriminator(2, 1, [hidden_dim] * d_num_layer)\n",
    "D_B = Discriminator(2, 1, [hidden_dim] * d_num_layer)\n",
    "\n",
    "G_AB.cuda()\n",
    "G_BA.cuda()\n",
    "D_A.cuda()\n",
    "D_B.cuda()\n",
    "\n",
    "# optimizer\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "d = nn.MSELoss()\n",
    "bce = nn.BCELoss()\n",
    "\n",
    "optimizer_d = torch.optim.Adam(\n",
    "    chain(D_A.parameters(), D_B.parameters()), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_g = torch.optim.Adam(\n",
    "    chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# training\n",
    "num_epoch = 50000\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "real_tensor = Variable(torch.FloatTensor(batch_size).cuda())\n",
    "_ = real_tensor.data.fill_(real_label)\n",
    "print(real_tensor.sum())\n",
    "\n",
    "fake_tensor = Variable(torch.FloatTensor(batch_size).cuda())\n",
    "_ = fake_tensor.data.fill_(fake_label)\n",
    "print(fake_tensor.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iteration = 50000\n",
    "idx = 0\n",
    "\n",
    "A_loader, B_loader = iter(A_train_loader), iter(B_train_loader)\n",
    "\n",
    "for idx in trange(max_iteration):\n",
    "    try:\n",
    "        x_A, x_B = A_loader.next()[0], B_loader.next()[0]\n",
    "    except StopIteration:\n",
    "        A_loader, B_loader = iter(A_train_loader), iter(B_train_loader)\n",
    "        x_A, x_B = A_loader.next()[0], B_loader.next()[0]\n",
    "\n",
    "    x_A, x_B = Variable(x_A.cuda()), Variable(x_B.cuda())\n",
    "    batch_size = x_A.size(0)\n",
    "\n",
    "    real_tensor.data.resize_(batch_size).fill_(real_label)\n",
    "    fake_tensor.data.resize_(batch_size).fill_(fake_label)\n",
    "\n",
    "    # update D network\n",
    "    D_A.zero_grad()\n",
    "    D_B.zero_grad()\n",
    "\n",
    "    x_AB = G_AB(x_A).detach()\n",
    "    x_BA = G_BA(x_B).detach()\n",
    "\n",
    "    x_ABA = G_BA(x_AB).detach()\n",
    "    x_BAB = G_AB(x_BA).detach()\n",
    "\n",
    "    l_d_A_real, l_d_A_fake = bce(D_A(x_A), real_tensor), bce(D_A(x_BA), fake_tensor)\n",
    "    l_d_B_real, l_d_B_fake = bce(D_B(x_B), real_tensor), bce(D_B(x_AB), fake_tensor)\n",
    "\n",
    "    l_d_A = l_d_A_real + l_d_A_fake\n",
    "    l_d_B = l_d_B_real + l_d_B_fake\n",
    "\n",
    "    l_d = l_d_A + l_d_B\n",
    "\n",
    "    l_d.backward()\n",
    "    optimizer_d.step()\n",
    "\n",
    "    # update G network\n",
    "    G_AB.zero_grad()\n",
    "    G_BA.zero_grad()\n",
    "\n",
    "    x_AB = G_AB(x_A)\n",
    "    x_BA = G_BA(x_B)\n",
    "\n",
    "    x_ABA = G_BA(x_AB)\n",
    "    x_BAB = G_AB(x_BA)\n",
    "\n",
    "    l_const_A = d(x_ABA, x_A)\n",
    "    l_const_B = d(x_BAB, x_B)\n",
    "\n",
    "    l_gan_A = bce(D_A(x_BA), real_tensor)\n",
    "    l_gan_B = bce(D_B(x_AB), real_tensor)\n",
    "\n",
    "    l_g = l_gan_A + l_gan_B + l_const_A + l_const_B\n",
    "\n",
    "    l_g.backward()\n",
    "    optimizer_g.step()\n",
    "\n",
    "    if idx % (max_iteration/20) == 0:\n",
    "        #print(\"[{}/{}] Loss_D: {:.4f} Loss_G: {:.4f}\". \\\n",
    "        #      format(idx, max_iteration, l_d.data[0], l_g.data[0]))\n",
    "\n",
    "        #print(\"[{}/{}] l_d_A_real: {:.4f} l_d_A_fake: {:.4f}, l_d_B_real: {:.4f}, l_d_B_fake: {:.4f}\". \\\n",
    "        #      format(idx, max_iteration, l_d_A_real.data[0], l_d_A_fake.data[0],  \n",
    "        #             l_d_B_real.data[0], l_d_B_fake.data[0]))\n",
    "\n",
    "        #print(\"[{}/{}] l_const_A: {:.4f} l_const_B: {:.4f}, l_gan_A: {:.4f}, l_gan_B: {:.4f}\". \\\n",
    "        #      format(idx, max_iteration, l_const_A.data[0], l_const_B.data[0],  \n",
    "        #             l_gan_A.data[0], l_gan_B.data[0]))\n",
    "\n",
    "        plt.figure()\n",
    "        ax = sns.kdeplot(B_test_np[:1000],\n",
    "                         cmap=\"Reds\", shade=True, shade_lowest=False)\n",
    "\n",
    "        plot(B_test_np[:1000], 'k.')\n",
    "        for key, value in A_data_with_class.items():\n",
    "            data = torch.from_numpy(value[:1000]).float()\n",
    "            pred = G_AB(Variable(data).cuda()).data.cpu().numpy()\n",
    "            plot(pred, '.')\n",
    "\n",
    "        plt.figure()\n",
    "        ax = sns.kdeplot(A_test_np[:1000],\n",
    "                         cmap=\"Blues\", shade=True, shade_lowest=False)\n",
    "        \n",
    "        plot(A_test_np[:1000], 'k.')\n",
    "        for key, value in B_data_with_class.items():\n",
    "            data = torch.from_numpy(value[:1000]).float()\n",
    "            pred = G_BA(Variable(data).cuda()).data.cpu().numpy()\n",
    "            plot(pred, '.')\n",
    "    \n",
    "        #ax = sns.kdeplot(G_AB(Variable(A_test[:1000]).cuda()).data.cpu().numpy(),\n",
    "        #     cmap=\"Reds\", shade=True, shade_lowest=False)\n",
    "        #ax = sns.kdeplot(G_BA(Variable(B_test[:1000]).cuda()).data.cpu().numpy(),\n",
    "        #                 cmap=\"Blues\", shade=True, shade_lowest=False)\n",
    "\n",
    "        plt.show()\n",
    "        plt.pause(0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
